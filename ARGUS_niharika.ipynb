{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAWZoUZp7b3Y"
      },
      "source": [
        "# 1. PROBLEM STATEMENT\n",
        "To create a multiple classification model that can classify the crime and normal incidents when exposed to such, as one of the 14 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGaiE0BW_ao"
      },
      "source": [
        "# 2. DATA\n",
        "The data is taken from Kaggle, named, \"UCF Crime Dataset\". It is an extensive dataset with huge amount of videos in each of the 14 classes; 13 with anomaly videos and 1 with normal videos. The dataset folder also contains certain .docx files regarding the structure of dataset and a readme.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogDV7OYlZtLM",
        "outputId": "d4188474-d464-4778-c6ca-81295aad65bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13M_zlTjoPg9",
        "outputId": "df1bfcc9-8119-44a4-c5b0-c16887d6a5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg1Ka91pSRZA",
        "outputId": "dda0272d-4357-4c2a-cec7-b4b7e4dbdaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            50Gi       1.6Gi        45Gi       3.0Mi       4.0Gi        48Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_KHW0b6lyE"
      },
      "source": [
        "# 3. EVALUATION\n",
        "The model will be evaluated on a separate test dataset using accuracy, precision, recall, and F1-score to measure its effectiveness in detecting and classifying violent activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthwMNAK747-"
      },
      "source": [
        "# 4. FEATURES\n",
        "Each video will be decomposed into frames that will serve as the model’s primary features.\n",
        "The extracted features should include spatial information (visual appearance, object positions) and temporal information (motion patterns across frames).\n",
        "Together, these features enable the CNN–LSTM model to learn both what is happening in a frame and how it evolves over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4zcfpVt8_BN"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeuW54uUatFx"
      },
      "outputs": [],
      "source": [
        "# Set Configurations\n",
        "BASE_DIR= Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)\")\n",
        "\n",
        "FRAMES_DIR= BASE_DIR/\"Frames\"\n",
        "SPLIT_RATIO= 0.8 #rest 20% for testimg\n",
        "FRAME_SIZE= (224, 224)\n",
        "FRAME_SKIP= 5\n",
        "\n",
        "## Source Folders\n",
        "SOURCE_FOLDER1 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-1\"/ \"Anomaly-Videos-Part-1\",\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-2\"/ \"Anomaly-Videos-Part-2\"]\n",
        "SOURCE_FOLDER2 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-3\"/ \"Anomaly-Videos-Part-3\",\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-4\"/ \"Anomaly-Videos-Part-4\"]\n",
        "SOURCE_FOLDER3 = [\n",
        "    BASE_DIR / \"Normal_Videos_for_Event_Recognition\",\n",
        "    BASE_DIR / \"Testing_Normal_Videos_Anomaly\"/ \"Testing_Normal_Videos_Anomaly\"]\n",
        "SOURCE_FOLDER4 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part_5\"\n",
        "]\n",
        "SOURCE_FOLDER5= [\n",
        "    BASE_DIR / \"Explosion\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqIg2HiRnZb7"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "def extract_frames(video_path, output_folder, frame_skip=FRAME_SKIP):\n",
        "  \"\"\"\n",
        "  Extracts frames from a single video into output folder.\n",
        "  \"\"\"\n",
        "  cap= cv2.VideoCapture(str(video_path))\n",
        "  if not cap.isOpened():\n",
        "    print(f\"Could not open {video_path}\")\n",
        "    return\n",
        "\n",
        "  frame_count= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "  frame_idx= 0\n",
        "  saved= 0\n",
        "\n",
        "  while True:\n",
        "    ret, frame= cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if frame_idx % frame_skip == 0:\n",
        "      frame= cv2.resize(frame, FRAME_SIZE)\n",
        "      frame_file= output_folder/f\"frame_{saved:05d}.jpg\"\n",
        "      cv2.imwrite(str(frame_file), frame)\n",
        "      saved += 1\n",
        "    frame_idx += 1\n",
        "  cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3MgsqbQKpOf",
        "outputId": "b7f1c30c-9553-4134-d0d1-63d499c45742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Found 4 classes:\n",
            "Abuse: 51 videos\n",
            "Arrest: 50 videos\n",
            "Arson: 50 videos\n",
            "Assault: 50 videos\n",
            "\n",
            " Abuse: 40 train, 11 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Abuse (train): 100%|██████████| 40/40 [08:13<00:00, 12.33s/it]\n",
            "Extracting Abuse (test): 100%|██████████| 11/11 [05:30<00:00, 30.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Arrest: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Arrest (train): 100%|██████████| 40/40 [18:47<00:00, 28.19s/it]\n",
            "Extracting Arrest (test): 100%|██████████| 10/10 [02:15<00:00, 13.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Arson: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Arson (train): 100%|██████████| 40/40 [18:10<00:00, 27.27s/it]\n",
            "Extracting Arson (test): 100%|██████████| 10/10 [01:41<00:00, 10.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Assault: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Assault (train): 100%|██████████| 40/40 [07:40<00:00, 11.50s/it]\n",
            "Extracting Assault (test): 100%|██████████| 10/10 [01:54<00:00, 11.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER1:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXsIoaT93z9M",
        "outputId": "df75c596-5611-4eb5-9443-327748b9b08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Found 6 classes:\n",
            "RoadAccidents: 150 videos\n",
            "Robbery: 150 videos\n",
            "Shooting: 50 videos\n",
            "Shoplifting: 50 videos\n",
            "Stealing: 100 videos\n",
            "Vandalism: 50 videos\n",
            "\n",
            " RoadAccidents: 120 train, 30 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting RoadAccidents (train): 100%|██████████| 120/120 [10:48<00:00,  5.40s/it]\n",
            "Extracting RoadAccidents (test): 100%|██████████| 30/30 [03:25<00:00,  6.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Robbery: 120 train, 30 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Robbery (train): 100%|██████████| 120/120 [21:02<00:00, 10.52s/it]\n",
            "Extracting Robbery (test): 100%|██████████| 30/30 [06:00<00:00, 12.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shooting: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Shooting (train): 100%|██████████| 40/40 [07:26<00:00, 11.16s/it]\n",
            "Extracting Shooting (test): 100%|██████████| 10/10 [01:39<00:00,  9.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shoplifting: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Shoplifting (train): 100%|██████████| 40/40 [16:29<00:00, 24.74s/it]\n",
            "Extracting Shoplifting (test): 100%|██████████| 10/10 [04:08<00:00, 24.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Stealing: 80 train, 20 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Stealing (train): 100%|██████████| 80/80 [21:50<00:00, 16.38s/it]\n",
            "Extracting Stealing (test): 100%|██████████| 20/20 [06:50<00:00, 20.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Vandalism: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Vandalism (train): 100%|██████████| 40/40 [07:15<00:00, 10.89s/it]\n",
            "Extracting Vandalism (test): 100%|██████████| 10/10 [02:21<00:00, 14.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER2:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8rSbImD334F",
        "outputId": "e92861f0-dca0-44b3-b66a-8f5b1a8e6e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Found 1 classes:\n",
            "Normal: 50 videos\n",
            "\n",
            " Normal: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Normal (train): 100%|██████████| 40/40 [10:24<00:00, 15.61s/it]\n",
            "Extracting Normal (test): 100%|██████████| 10/10 [00:55<00:00,  5.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER3:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0rmxv-yvy4u",
        "outputId": "89540ed2-1c7a-4ace-8da5-a035350636c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Found 2 classes:\n",
            "Burglary: 50 videos\n",
            "Fight: 50 videos\n",
            "\n",
            " Burglary: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Burglary (train): 100%|██████████| 40/40 [11:58<00:00, 17.96s/it]\n",
            "Extracting Burglary (test): 100%|██████████| 10/10 [02:00<00:00, 12.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Fight: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Fight (train): 100%|██████████| 40/40 [15:27<00:00, 23.20s/it]\n",
            "Extracting Fight (test): 100%|██████████| 10/10 [05:26<00:00, 32.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER4:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"FightingA\" in class_dir.name:\n",
        "        label= \"Fight\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBFDLDJL7G1u",
        "outputId": "bcfbcc00-6dd5-4ebc-f0e1-5fe10a11fcbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Found 2 classes:\n",
            "Explosion: 49 videos\n",
            "Shooting: 50 videos\n",
            "\n",
            " Explosion: 39 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Explosion (train): 100%|██████████| 39/39 [07:35<00:00, 11.69s/it]\n",
            "Extracting Explosion (test): 100%|██████████| 10/10 [01:07<00:00,  6.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shooting: 40 train, 10 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Shooting (train): 100%|██████████| 40/40 [1:06:21<00:00, 99.53s/it] \n",
            "Extracting Shooting (test): 100%|██████████| 10/10 [23:20<00:00, 140.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER5:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TkEHwZ227O4"
      },
      "outputs": [],
      "source": [
        "import shutil, os\n",
        "shutil.rmtree(FRAMES_DIR / \"train\"/ \"Normal\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5ATVndgbbxV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "IMG_SIZE= (224, 224)\n",
        "SEQUENCE_LENGTH= 100 #frames per clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApZcmYTz-WCv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5NiOJXm7px2"
      },
      "outputs": [],
      "source": [
        "FRAMES_DIR = Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\")   # where your frames live: /content/frames/train, /content/frames/test\n",
        "OUT_DIR    = Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS\")  # where to save tfrecords (Drive OK, few files)\n",
        "SEQ_LEN    = 100   # frames per clip (pad/trunc)\n",
        "IMG_SIZE   = (224,224)  # resize\n",
        "compress_jpeg = True    # store frames as jpeg bytes inside TFRecord (reduces disk & TFRecord size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arm8x0pH8Hyv"
      },
      "outputs": [],
      "source": [
        "OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Obga-P8eJD"
      },
      "outputs": [],
      "source": [
        "def _bytes_feature(b): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[b]))\n",
        "def _int64_feature(i): return tf.train.Feature(int64_list=tf.train.Int64List(value=[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUS0sXDZ9xGD",
        "outputId": "5fdf32d8-3ab6-4d08-d00d-e7d948cfa21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected classes: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fight', 'Normal', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
          ]
        }
      ],
      "source": [
        "# collect classes from train folder\n",
        "train_dir = FRAMES_DIR / \"train\"\n",
        "classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "print(\"Detected classes:\", classes)\n",
        "\n",
        "# save classes mapping for later\n",
        "np.save(str(OUT_DIR / \"classes.npy\"), np.array(classes))\n",
        "\n",
        "label_encoder = LabelEncoder().fit(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uUCagyj9-Re"
      },
      "outputs": [],
      "source": [
        "def clip_to_tfrecord(clip_path, label_int):\n",
        "    \"\"\"Serialize one clip folder to a tf.train.Example\"\"\"\n",
        "    files = sorted([f for f in os.listdir(clip_path) if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "    frames_bytes = []\n",
        "\n",
        "    for fname in files[:SEQ_LEN]:\n",
        "        img = cv2.imread(str(clip_path / fname))\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, IMG_SIZE)\n",
        "        if compress_jpeg:\n",
        "            ok, enc = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
        "            if not ok:\n",
        "                continue\n",
        "            frames_bytes.append(enc.tobytes())\n",
        "        else:\n",
        "            # raw bytes (float32 is big) -> store uint8 raw\n",
        "            frames_bytes.append(img.tobytes())\n",
        "\n",
        "    # pad by repeating last frame if needed\n",
        "    if len(frames_bytes) == 0:\n",
        "        return None\n",
        "    while len(frames_bytes) < SEQ_LEN:\n",
        "        frames_bytes.append(frames_bytes[-1])\n",
        "\n",
        "    feature = {\n",
        "        \"label\": _int64_feature(int(label_int)),\n",
        "        \"num_frames\": _int64_feature(len(frames_bytes)),\n",
        "        \"height\": _int64_feature(IMG_SIZE[0]),\n",
        "        \"width\": _int64_feature(IMG_SIZE[1]),\n",
        "        \"channels\": _int64_feature(3),\n",
        "        \"clip_name\": _bytes_feature(str(clip_path.name).encode(\"utf-8\")),\n",
        "        # frames as repeated bytes; store concatenated with separator? TF supports bytes_list\n",
        "        \"frames\": tf.train.Feature(bytes_list=tf.train.BytesList(value=frames_bytes)),\n",
        "    }\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW6F6qF5-G8z"
      },
      "outputs": [],
      "source": [
        "# function to write TFRecord for one split and one class\n",
        "def write_class_tfrecord(split, cls):\n",
        "    class_dir = FRAMES_DIR / split / cls\n",
        "    if not class_dir.exists():\n",
        "        print(\"Missing:\", class_dir); return\n",
        "    out_file = OUT_DIR / f\"{split}_{cls}.tfrecord\"\n",
        "    if out_file.exists():\n",
        "        print(f\"Skipping (exists): {out_file}\")\n",
        "        return\n",
        "    writer = tf.io.TFRecordWriter(str(out_file))\n",
        "    label_int = int(label_encoder.transform([cls])[0])\n",
        "    clips = sorted([p for p in class_dir.iterdir() if p.is_dir()])\n",
        "    print(f\"Writing {out_file}  ({len(clips)} clips)\")\n",
        "    for clip in tqdm(clips):\n",
        "        ex = clip_to_tfrecord(clip, label_int)\n",
        "        if ex is None:\n",
        "            continue\n",
        "        writer.write(ex.SerializeToString())\n",
        "    writer.close()\n",
        "    print(\"Saved:\", out_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nQxY5dr-S6T",
        "outputId": "63eb5271-90ce-4c17-a8a1-947f9e4604a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Abuse.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Arrest.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Arson.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Assault.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Burglary.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Explosion.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Fight.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Normal.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_RoadAccidents.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Robbery.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Shooting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Shoplifting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Stealing.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Vandalism.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Abuse.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Arrest.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Arson.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Assault.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Burglary.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Explosion.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Fight.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Normal.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_RoadAccidents.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Robbery.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Shooting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Shoplifting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Stealing.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Vandalism.tfrecord\n",
            "All TFRecords created in: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS\n",
            "Saved class list to: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/classes.npy\n"
          ]
        }
      ],
      "source": [
        "# Write records class-by-class for both splits\n",
        "for split in (\"train\",\"test\"):\n",
        "    for cls in classes:\n",
        "        write_class_tfrecord(split, cls)\n",
        "\n",
        "print(\"All TFRecords created in:\", OUT_DIR)\n",
        "print(\"Saved class list to:\", OUT_DIR / \"classes.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofm29hkpxZED"
      },
      "source": [
        "## High-level flow:\n",
        "\n",
        "1. Read TFRecords with tf.data (parse, decode JPEG bytes → frames tensor).\n",
        "\n",
        "2. Batch & shuffle with tf.data so training reads only small chunks into RAM.\n",
        "\n",
        "3. Apply light augmentation (optional, frame-wise or temporal).\n",
        "\n",
        "4. Model: per-frame CNN (feature extractor) wrapped with TimeDistributed → temporal model (LSTM or GRU) → attention → dense classifier.\n",
        "\n",
        "5. Train with model.fit() on the tf.data datasets, use callbacks (checkpoint, reduce LR).\n",
        "\n",
        "6. Save final weights and small artifacts (classes.npy) to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ME-NvS-KdPC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1PPuzw8xlt6"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 100\n",
        "IMG_H, IMG_W = 224, 224\n",
        "AUTOTUNE= tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFBLkkpays5q"
      },
      "outputs": [],
      "source": [
        "feature_desc= {\n",
        "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"num_frames\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"channels\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"clip_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"frames\": tf.io.VarLenFeature(tf.string)  # list of jpeg bytes\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PzBZbJT2q63"
      },
      "outputs": [],
      "source": [
        "def _parse_example(serialized):\n",
        "  ex= tf.io.parse_single_example(serialized, feature_desc)\n",
        "  frames_sparse= ex[\"frames\"] #SparseTensor of bytes\n",
        "  frames= tf.sparse.to_dense(frames_sparse, default_value=b'')\n",
        "  # decode each jpeg\n",
        "  def decode_fn(b):\n",
        "    img= tf.image.decode_jpeg(b, channels=3)\n",
        "    img= tf.image.resize(img, [IMG_H, IMG_W])\n",
        "    img= tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "  frames= tf.map_fn(decode_fn, frames, dtype= tf.float32)\n",
        "  frames= tf.reshape(frames, (SEQ_LEN, IMG_H, IMG_W, 3))\n",
        "  label= tf.cast(ex[\"label\"], tf.int32)\n",
        "  return frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK3wJQvIfj8E"
      },
      "outputs": [],
      "source": [
        "# 3. Build train/val datasets with batching and prefetching\n",
        "def make_dataset(tfrecord_paths, num_classes, batch_size=1, shuffle_buffer=256, training=True):\n",
        "    ds = tf.data.TFRecordDataset(tfrecord_paths, num_parallel_reads=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(_parse_example, num_parallel_calls=AUTOTUNE)\n",
        "    # OPTIONAL: augment only training data\n",
        "    if training:\n",
        "        ds = ds.map(lambda x, y: augment(x, y), num_parallel_calls=AUTOTUNE)\n",
        "    # convert int label → one-hot\n",
        "    ds = ds.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    # IMPORTANT for multi-epoch stability\n",
        "    if training:\n",
        "      ds = ds.repeat()\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAGFzc85szkj"
      },
      "outputs": [],
      "source": [
        "def augment(frames, label):\n",
        "\n",
        "    # apply augmentation per frame (not to entire sequence)\n",
        "    def aug_single(img):\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, 0.12)\n",
        "        img = tf.image.random_contrast(img, 0.9, 1.1)\n",
        "        img = tf.image.random_crop(img, size=[IMG_H-8, IMG_W-8, 3])\n",
        "        img = tf.image.resize(img, [IMG_H, IMG_W])\n",
        "        return img\n",
        "\n",
        "    frames = tf.map_fn(aug_single, frames)   # apply to each frame\n",
        "    return frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvVlPUWh7vMK",
        "outputId": "0f50553f-4947-47a0-c6b8-a83795e5bc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TFRecords: 14\n",
            "Val TFRecords: 14\n"
          ]
        }
      ],
      "source": [
        "classes= np.load(OUT_DIR/ \"classes.npy\")\n",
        "num_classes= len(classes)\n",
        "train_paths= [str(p) for p in OUT_DIR.glob(\"train_*.tfrecord\")]\n",
        "val_paths= [str(p) for p in OUT_DIR.glob(\"test_*tfrecord\")]\n",
        "batch_size = 1\n",
        "\n",
        "train_ds = make_dataset(train_paths,num_classes, batch_size=batch_size, training=True)\n",
        "val_ds   = make_dataset(val_paths, num_classes, batch_size=batch_size, training=False)\n",
        "\n",
        "print(\"Train TFRecords:\", len(train_paths))\n",
        "print(\"Val TFRecords:\", len(val_paths))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04A5T8g4JbCF"
      },
      "source": [
        "## ARGUS Architecture\n",
        "Pre-filter (Motion Thresholding & Person Count)  --> CNN (mobilenetV2) --> LSTM (BiLSTM) --> Spatial + Temporal Attention --> Dense Layer (final output of prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJNsgSuq0UUs"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN=100\n",
        "batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPncvLfHu5pT"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session() #(if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "L0MwLIoSCkVc",
        "outputId": "c68a42c5-a1dc-4d4b-cf13-654db15feee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ARGUS_ViolenceDetector\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ARGUS_ViolenceDetector\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │  \u001b[38;5;34m2,257,984\u001b[0m │ frames[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m655,872\u001b[0m │ frame_cnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ frame_fc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ bilstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m262,656\u001b[0m │ bilstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m65,664\u001b[0m │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │        \u001b[38;5;34m129\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mSoftmax\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m1\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ attn_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │      \u001b[38;5;34m1,806\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ frames[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ frame_cnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ frame_fc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ bilstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ bilstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ attn_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,806</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,559,184\u001b[0m (25.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,559,184</span> (25.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,301,200\u001b[0m (16.41 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,301,200</span> (16.41 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_argus_model(seq_len=SEQ_LEN, img_size=(IMG_H, IMG_W), num_classes=num_classes, feature_dim= 512):\n",
        "   # 1) Base CNN: MobileNetV2 (frame-level feature extractor)\n",
        "  base_cnn= MobileNetV2(\n",
        "      include_top= False,\n",
        "      pooling= \"avg\",\n",
        "      input_shape= (img_size[0], img_size[1], 3),\n",
        "      weights= \"imagenet\"\n",
        "  )\n",
        "  base_cnn.trainable= False  # start frozen; fine-tune later\n",
        "\n",
        "  # 2) Input: sequence of frames\n",
        "  frames_in= layers.Input(shape=(seq_len, img_size[0], img_size[1], 3), name=\"frames\")\n",
        "\n",
        "  # 3) TimeDistributed CNN: apply MobileNetV2 to each frame\n",
        "  x = layers.TimeDistributed(base_cnn, name=\"frame_cnn\")(frames_in)  # (B, T, cnn_feat_dim)\n",
        "\n",
        "  # project features to lower dimension\n",
        "  x = layers.TimeDistributed(layers.Dense(feature_dim, activation='relu'), name=\"frame_fc\")(x)\n",
        "\n",
        "  # 4) Temporal Modelling using BiLSTM\n",
        "  x = layers.Bidirectional(\n",
        "       layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
        "        name=\"bilstm_1\")(x)                   # (B, T, 512)\n",
        "\n",
        "  x = layers.Bidirectional(\n",
        "       layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
        "        name=\"bilstm_2\")(x)\n",
        "\n",
        "  # optional projection (to reduce)\n",
        "  x = layers.TimeDistributed(layers.Dense(512, activation='relu'))(x)\n",
        "\n",
        "  # 5) Attention over time (Temporal Attention)\n",
        "  attn_dim= 128\n",
        "\n",
        "  # learn an attention score for each timestep\n",
        "  attn_scores= layers.Dense(attn_dim, activation=\"tanh\")(x)  # (B, T, attn_dim)\n",
        "  attn_scores= layers.Dense(1, activation=None)(attn_scores) # (B, T, 1)\n",
        "\n",
        "  # turn scores into weights\n",
        "  attn_weights= layers.Softmax(axis=1, name=\"attn_weights\")(attn_scores)   # (B, T, 1)\n",
        "\n",
        "  # weighted sum of LSTM outputs\n",
        "  x = layers.Multiply()([x, attn_weights])                       # (B, T, feat)\n",
        "\n",
        "  # attention-weighted temporal pooling (NO Lambda cuz it doesn't let us load our best model in phase 2)\n",
        "  x = layers.Attention(use_scale=True)([x, x])   # (B, T, feat)\n",
        "  x = layers.GlobalAveragePooling1D(name=\"attn_pool\")(x)  # (B, feat)\n",
        "\n",
        "  # 6) Classification Model\n",
        "  x = layers.LayerNormalization()(x)\n",
        "  x = layers.Dense(256, activation='gelu')(x)\n",
        "  x = layers.Dropout(0.4)(x)\n",
        "  x = layers.Dense(128, activation='gelu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "\n",
        "  model = Model(frames_in, out, name=\"ARGUS_ViolenceDetector\")\n",
        "  return model\n",
        "\n",
        "model = build_argus_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxqaHtLnyVd7"
      },
      "source": [
        "## Training Cell\n",
        "(PHASE 1: Backbone Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLPtUHkey5ap"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29z_0PTpy7kB"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR= \"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "checkpoint_path = os.path.join(SAVE_DIR, \"argus_best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES66ohqY0H6q"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks=[\n",
        "    ModelCheckpoint(\n",
        "      checkpoint_path,\n",
        "      monitor=\"val_loss\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only= False,\n",
        "      verbose=1\n",
        " ),\n",
        "    EarlyStopping(\n",
        "      monitor=\"val_loss\",\n",
        "      patience=5,\n",
        "      restore_best_weights=True\n",
        " ),\n",
        "    ReduceLROnPlateau(\n",
        "      monitor=\"val_loss\",\n",
        "      factor=0.3,\n",
        "      patience=3,\n",
        "      verbose=1\n",
        " ),\n",
        "    ModelCheckpoint(\n",
        "      filepath=checkpoint_path.replace(\".h5\", \"_bestacc.h5\"),\n",
        "      monitor=\"val_accuracy\",\n",
        "      mode=\"max\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only=False,\n",
        "      verbose=1\n",
        ")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyaDozfNU3VK",
        "outputId": "9e9436c0-4684-4161-a590-b273d48f041b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1299 - loss: 2.5190\n",
            "Epoch 1: val_loss improved from inf to 2.79735, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1542s\u001b[0m 2s/step - accuracy: 0.1299 - loss: 2.5189 - val_accuracy: 0.1047 - val_loss: 2.7973 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1515 - loss: 2.4912\n",
            "Epoch 2: val_loss improved from 2.79735 to 2.79514, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 1s/step - accuracy: 0.1515 - loss: 2.4911 - val_accuracy: 0.1047 - val_loss: 2.7951 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1837 - loss: 2.4054\n",
            "Epoch 3: val_loss improved from 2.79514 to 2.50456, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 1s/step - accuracy: 0.1838 - loss: 2.4053 - val_accuracy: 0.1728 - val_loss: 2.5046 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2621 - loss: 2.3181\n",
            "Epoch 4: val_loss did not improve from 2.50456\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1025s\u001b[0m 1s/step - accuracy: 0.2622 - loss: 2.3181 - val_accuracy: 0.2199 - val_loss: 2.5681 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2786 - loss: 2.2539\n",
            "Epoch 5: val_loss did not improve from 2.50456\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 1s/step - accuracy: 0.2786 - loss: 2.2538 - val_accuracy: 0.1780 - val_loss: 2.7052 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2874 - loss: 2.2178\n",
            "Epoch 6: val_loss improved from 2.50456 to 2.46040, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1017s\u001b[0m 1s/step - accuracy: 0.2874 - loss: 2.2177 - val_accuracy: 0.2251 - val_loss: 2.4604 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3040 - loss: 2.1499\n",
            "Epoch 7: val_loss improved from 2.46040 to 2.42790, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1026s\u001b[0m 1s/step - accuracy: 0.3041 - loss: 2.1500 - val_accuracy: 0.2775 - val_loss: 2.4279 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3570 - loss: 2.0341\n",
            "Epoch 8: val_loss improved from 2.42790 to 2.41085, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1018s\u001b[0m 1s/step - accuracy: 0.3570 - loss: 2.0342 - val_accuracy: 0.2880 - val_loss: 2.4108 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2744 - loss: 2.1357\n",
            "Epoch 9: val_loss improved from 2.41085 to 2.36650, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1017s\u001b[0m 1s/step - accuracy: 0.2745 - loss: 2.1357 - val_accuracy: 0.2827 - val_loss: 2.3665 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3553 - loss: 1.9670\n",
            "Epoch 10: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 1s/step - accuracy: 0.3552 - loss: 1.9671 - val_accuracy: 0.2775 - val_loss: 2.8784 - learning_rate: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=721,\n",
        "    validation_steps=191,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "id": "XtRjSU_wSvoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemO-UEuhr1F"
      },
      "source": [
        "Phase A’s job is:\n",
        "\n",
        "**“Does the system learn anything meaningful end-to-end?”**\n",
        "\n",
        "Answer is **\"YES\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYaC4sRc2WOM"
      },
      "source": [
        "(Phase 2: Unfreeze backbone + fine-tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcLLZfheQldx"
      },
      "outputs": [],
      "source": [
        "train_paths = [str(p) for p in OUT_DIR.glob(\"train_*.tfrecord\")]\n",
        "val_paths   = [str(p) for p in OUT_DIR.glob(\"test_*.tfrecord\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGyaqalMQxOE",
        "outputId": "edeb6cf7-8e76-4ed4-d514-5fcd44a58199"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-1354474609.py:8: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.ignore_errors` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 721\n",
            "Val examples: 191\n",
            "Steps per epoch: 360\n",
            "Val steps: 95\n"
          ]
        }
      ],
      "source": [
        "def count_examples_safe(tfrecord_paths):\n",
        "    total = 0\n",
        "\n",
        "    for p in tfrecord_paths:\n",
        "        ds = tf.data.TFRecordDataset(p)\n",
        "\n",
        "        # THIS is the correct way in your TF version\n",
        "        ds = ds.apply(tf.data.experimental.ignore_errors())\n",
        "\n",
        "        for _ in ds:\n",
        "            total += 1\n",
        "\n",
        "    return total\n",
        "\n",
        "\n",
        "train_examples = count_examples_safe(train_paths)\n",
        "val_examples   = count_examples_safe(val_paths)\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "STEPS_PER_EPOCH = train_examples // BATCH_SIZE\n",
        "VAL_STEPS       = val_examples // BATCH_SIZE\n",
        "\n",
        "print(\"Train examples:\", train_examples)\n",
        "print(\"Val examples:\", val_examples)\n",
        "print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n",
        "print(\"Val steps:\", VAL_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98EdRbU04LJN",
        "outputId": "c99720b9-dc63-4c68-b668-54eed0d022c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted files in Model_Trained folder:\n",
            " argus_best_model.h5  'Model_Trained_(given)'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Mounted files in Model_Trained folder:\")\n",
        "!ls \"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UTEnV7FV7-N0",
        "outputId": "6bad113c-b8a2-4192-bec6-b5f3aa829099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfrozen last 20 layers (except BatchNorm)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ARGUS_ViolenceDetector\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ARGUS_ViolenceDetector\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │  \u001b[38;5;34m2,257,984\u001b[0m │ frames[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m655,872\u001b[0m │ frame_cnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ frame_fc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ bilstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m262,656\u001b[0m │ bilstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m65,664\u001b[0m │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │        \u001b[38;5;34m129\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mSoftmax\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m1\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ attn_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │      \u001b[38;5;34m1,806\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ frames[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ frame_cnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ frame_fc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ bilstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ bilstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ attn_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,806</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,559,186\u001b[0m (25.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,559,186</span> (25.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,342,480\u001b[0m (20.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,342,480</span> (20.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,216,704\u001b[0m (4.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,216,704</span> (4.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2675 - loss: 2.2331\n",
            "Epoch 1: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1600s\u001b[0m 2s/step - accuracy: 0.2676 - loss: 2.2330 - val_accuracy: 0.2827 - val_loss: 2.4108 - learning_rate: 3.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2525 - loss: 2.2625\n",
            "Epoch 2: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1061s\u001b[0m 1s/step - accuracy: 0.2525 - loss: 2.2624 - val_accuracy: 0.2880 - val_loss: 2.3795 - learning_rate: 3.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3013 - loss: 2.2148\n",
            "Epoch 3: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1061s\u001b[0m 1s/step - accuracy: 0.3013 - loss: 2.2148 - val_accuracy: 0.2513 - val_loss: 2.4124 - learning_rate: 3.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3486 - loss: 2.1419\n",
            "Epoch 4: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1125s\u001b[0m 2s/step - accuracy: 0.3486 - loss: 2.1419 - val_accuracy: 0.2565 - val_loss: 2.4767 - learning_rate: 3.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3063 - loss: 2.1962\n",
            "Epoch 5: val_loss did not improve from 2.36650\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1101s\u001b[0m 2s/step - accuracy: 0.3063 - loss: 2.1961 - val_accuracy: 0.2042 - val_loss: 2.6291 - learning_rate: 3.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3544 - loss: 1.9958\n",
            "Epoch 6: val_loss did not improve from 2.36650\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 1s/step - accuracy: 0.3545 - loss: 1.9958 - val_accuracy: 0.2880 - val_loss: 2.4025 - learning_rate: 9.0000e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3618 - loss: 1.9978\n",
            "Epoch 7: val_loss improved from 2.36650 to 2.33609, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 1s/step - accuracy: 0.3618 - loss: 1.9978 - val_accuracy: 0.3037 - val_loss: 2.3361 - learning_rate: 9.0000e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4014 - loss: 1.8857\n",
            "Epoch 8: val_loss improved from 2.33609 to 2.32332, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 1s/step - accuracy: 0.4014 - loss: 1.8857 - val_accuracy: 0.3037 - val_loss: 2.3233 - learning_rate: 9.0000e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4115 - loss: 1.8408\n",
            "Epoch 9: val_loss did not improve from 2.32332\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 1s/step - accuracy: 0.4115 - loss: 1.8408 - val_accuracy: 0.2932 - val_loss: 2.4763 - learning_rate: 9.0000e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3852 - loss: 1.8339\n",
            "Epoch 10: val_loss did not improve from 2.32332\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 1s/step - accuracy: 0.3852 - loss: 1.8339 - val_accuracy: 0.2932 - val_loss: 2.4120 - learning_rate: 9.0000e-06\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = load_model(checkpoint_path)\n",
        "\n",
        "base_cnn = model.get_layer(\"frame_cnn\").layer\n",
        "\n",
        "# Freeze everything first\n",
        "for layer in base_cnn.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Unfreeze only last N layers\n",
        "N= 20\n",
        "for layer in base_cnn.layers[-N:]:\n",
        "  if not isinstance(layer, BatchNormalization):\n",
        "    layer.trainable=True\n",
        "\n",
        "print(\"Unfrozen last\", N, \"layers (except BatchNorm)\")\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=3e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=721,\n",
        "    validation_steps=191,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00sQ0JGjrxhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fa021e-cbc1-4a47-9f14-f5d647d5e279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted files in Model_Trained folder:\n",
            " argus_best_model_bestacc.h5   argus_best_model.h5  'Model_Trained_(given)'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Mounted files in Model_Trained folder:\")\n",
        "!ls \"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3: Temporal Refinement**"
      ],
      "metadata": {
        "id": "dotBFuz3reWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-requisite for Phase 3\n",
        "# Computing class weights\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_labels_from_tfrecord(tfrecord_paths):\n",
        "    labels = []\n",
        "\n",
        "    raw_dataset = tf.data.TFRecordDataset(tfrecord_paths)\n",
        "\n",
        "    for raw_record in raw_dataset:\n",
        "        example = tf.train.Example()\n",
        "        example.ParseFromString(raw_record.numpy())\n",
        "        label = example.features.feature[\"label\"].int64_list.value[0]\n",
        "        labels.append(label)\n",
        "\n",
        "    return labels\n",
        "\n",
        "labels = get_labels_from_tfrecord(train_paths)\n",
        "\n",
        "counts = Counter(labels)\n",
        "total = sum(counts.values())\n",
        "num_classes = len(counts)\n",
        "\n",
        "class_weights = {\n",
        "    i: total / (num_classes * counts[i]) for i in counts\n",
        "}\n",
        "\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uqd4CRfsZa4",
        "outputId": "e7ede015-ec5f-4caa-8da2-078094b108f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 1.355357142857143, 2: 1.355357142857143, 3: 1.355357142857143, 4: 1.355357142857143, 5: 1.39010989010989, 6: 1.355357142857143, 7: 1.355357142857143, 8: 0.4517857142857143, 9: 0.4517857142857143, 10: 1.355357142857143, 11: 1.355357142857143, 12: 0.6776785714285715, 13: 1.355357142857143, 0: 1.355357142857143}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Loading best model from Phase 2\n",
        "model= load_model(checkpoint_path)\n",
        "\n",
        "# Freezing all the CNN layers/ complete MobileNetV2\n",
        "base_cnn= model.get_layer(\"frame_cnn\").layer\n",
        "\n",
        "for layer in base_cnn.layers:\n",
        "  layer.trainable= False\n",
        "\n",
        "for layer in base_cnn.layers[-10:]:\n",
        "  if not isinstance(layer, BatchNormalization):\n",
        "    layer.trainable= True\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer= Adam(learning_rate=3e-5),\n",
        "    loss= \"categorical_crossentropy\",\n",
        "    metrics= [\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history= model.fit(\n",
        "    train_ds,\n",
        "    validation_data= val_ds,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=721,\n",
        "    validation_steps=191,\n",
        "    class_weight= class_weights,\n",
        "    callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qBU09oDylXxN",
        "outputId": "36dcb366-4a65-4990-e3d8-bc38e1fe2316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ARGUS_ViolenceDetector\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ARGUS_ViolenceDetector\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │  \u001b[38;5;34m2,257,984\u001b[0m │ frames[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m655,872\u001b[0m │ frame_cnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ frame_fc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,574,912\u001b[0m │ bilstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m262,656\u001b[0m │ bilstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m65,664\u001b[0m │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │        \u001b[38;5;34m129\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mSoftmax\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m1\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ attn_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │      \u001b[38;5;34m1,806\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ frames[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ frame_cnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ frame_fc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ bilstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ bilstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ time_distributed… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
              "│                     │                   │            │ attn_weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ attn_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,806</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,559,184\u001b[0m (25.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,559,184</span> (25.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,026,640\u001b[0m (19.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,026,640</span> (19.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,532,544\u001b[0m (5.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,532,544</span> (5.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3180 - loss: 2.2678\n",
            "Epoch 1: val_loss improved from inf to 2.43847, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.24607, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model_bestacc.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1617s\u001b[0m 2s/step - accuracy: 0.3181 - loss: 2.2675 - val_accuracy: 0.2461 - val_loss: 2.4385 - learning_rate: 3.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3604 - loss: 2.0789\n",
            "Epoch 2: val_loss did not improve from 2.43847\n",
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.24607\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1089s\u001b[0m 2s/step - accuracy: 0.3604 - loss: 2.0787 - val_accuracy: 0.2094 - val_loss: 2.6328 - learning_rate: 3.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3829 - loss: 1.9733\n",
            "Epoch 3: val_loss did not improve from 2.43847\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.24607 to 0.25131, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model_bestacc.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 2s/step - accuracy: 0.3829 - loss: 1.9732 - val_accuracy: 0.2513 - val_loss: 2.6178 - learning_rate: 3.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4058 - loss: 1.7850\n",
            "Epoch 4: val_loss did not improve from 2.43847\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 0.25131\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1088s\u001b[0m 2s/step - accuracy: 0.4058 - loss: 1.7850 - val_accuracy: 0.2042 - val_loss: 2.6477 - learning_rate: 3.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4618 - loss: 1.6904\n",
            "Epoch 5: val_loss did not improve from 2.43847\n",
            "\n",
            "Epoch 5: val_accuracy did not improve from 0.25131\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1089s\u001b[0m 2s/step - accuracy: 0.4618 - loss: 1.6904 - val_accuracy: 0.2408 - val_loss: 2.6073 - learning_rate: 9.0000e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4581 - loss: 1.5205\n",
            "Epoch 6: val_loss did not improve from 2.43847\n",
            "\n",
            "Epoch 6: val_accuracy improved from 0.25131 to 0.25654, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model_bestacc.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1089s\u001b[0m 2s/step - accuracy: 0.4581 - loss: 1.5206 - val_accuracy: 0.2565 - val_loss: 2.6413 - learning_rate: 9.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Mounted files in Model_Trained folder:\")\n",
        "!ls \"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjkrC6GRupMH",
        "outputId": "82c34601-931a-4f75-bca3-11aa17fd3b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted files in Model_Trained folder:\n",
            " argus_best_model_bestacc.h5   argus_best_model.h5  'Model_Trained_(given)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQsG2kA-z78s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
