{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PROBLEM STATEMENT\n",
        "To create a multiple classification model that can classify the crime and normal incidents when exposed to such, as one of the 14 classes."
      ],
      "metadata": {
        "id": "yAWZoUZp7b3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. DATA\n",
        "The data is taken from Kaggle, named, \"UCF Crime Dataset\". It is an extensive dataset with huge amount of videos in each of the 14 classes; 13 with anomaly videos and 1 with normal videos. The dataset folder also contains certain .docx files regarding the structure of dataset and a readme.txt file."
      ],
      "metadata": {
        "id": "nNGaiE0BW_ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "13M_zlTjoPg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff4d08c-aa6f-47d1-abe2-0db6c94cfeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. EVALUATION\n",
        "The model will be evaluated on a separate test dataset using accuracy, precision, recall, and F1-score to measure its effectiveness in detecting and classifying violent activities."
      ],
      "metadata": {
        "id": "wZ_KHW0b6lyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. FEATURES\n",
        "Each video will be decomposed into frames that will serve as the model’s primary features.\n",
        "The extracted features should include spatial information (visual appearance, object positions) and temporal information (motion patterns across frames).\n",
        "Together, these features enable the CNN–LSTM model to learn both what is happening in a frame and how it evolves over time."
      ],
      "metadata": {
        "id": "cthwMNAK747-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "metadata": {
        "id": "r4zcfpVt8_BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Configurations\n",
        "BASE_DIR= Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)\")\n",
        "\n",
        "FRAMES_DIR= BASE_DIR/\"Frames\"\n",
        "SPLIT_RATIO= 0.8 #rest 20% for testimg\n",
        "FRAME_SIZE= (224, 224)\n",
        "FRAME_SKIP= 5\n",
        "\n",
        "## Source Folders\n",
        "SOURCE_FOLDER1 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-1\"/ \"Anomaly-Videos-Part-1\",\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-2\"/ \"Anomaly-Videos-Part-2\"]\n",
        "SOURCE_FOLDER2 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-3\"/ \"Anomaly-Videos-Part-3\",\n",
        "    BASE_DIR / \"Anomaly-Videos-Part-4\"/ \"Anomaly-Videos-Part-4\"]\n",
        "SOURCE_FOLDER3 = [\n",
        "    BASE_DIR / \"Normal_Videos_for_Event_Recognition\",\n",
        "    BASE_DIR / \"Testing_Normal_Videos_Anomaly\"/ \"Testing_Normal_Videos_Anomaly\"]\n",
        "SOURCE_FOLDER4 = [\n",
        "    BASE_DIR / \"Anomaly-Videos-Part_5\"\n",
        "]\n",
        "SOURCE_FOLDER5= [\n",
        "    BASE_DIR / \"Explosion\",\n",
        "]"
      ],
      "metadata": {
        "id": "qeuW54uUatFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def extract_frames(video_path, output_folder, frame_skip=FRAME_SKIP):\n",
        "  \"\"\"\n",
        "  Extracts frames from a single video into output folder.\n",
        "  \"\"\"\n",
        "  cap= cv2.VideoCapture(str(video_path))\n",
        "  if not cap.isOpened():\n",
        "    print(f\"Could not open {video_path}\")\n",
        "    return\n",
        "\n",
        "  frame_count= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "  frame_idx= 0\n",
        "  saved= 0\n",
        "\n",
        "  while True:\n",
        "    ret, frame= cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if frame_idx % frame_skip == 0:\n",
        "      frame= cv2.resize(frame, FRAME_SIZE)\n",
        "      frame_file= output_folder/f\"frame_{saved:05d}.jpg\"\n",
        "      cv2.imwrite(str(frame_file), frame)\n",
        "      saved += 1\n",
        "    frame_idx += 1\n",
        "  cap.release()"
      ],
      "metadata": {
        "id": "UqIg2HiRnZb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER1:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3MgsqbQKpOf",
        "outputId": "b7f1c30c-9553-4134-d0d1-63d499c45742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found 4 classes:\n",
            "Abuse: 51 videos\n",
            "Arrest: 50 videos\n",
            "Arson: 50 videos\n",
            "Assault: 50 videos\n",
            "\n",
            " Abuse: 40 train, 11 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Abuse (train): 100%|██████████| 40/40 [08:13<00:00, 12.33s/it]\n",
            "Extracting Abuse (test): 100%|██████████| 11/11 [05:30<00:00, 30.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Arrest: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Arrest (train): 100%|██████████| 40/40 [18:47<00:00, 28.19s/it]\n",
            "Extracting Arrest (test): 100%|██████████| 10/10 [02:15<00:00, 13.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Arson: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Arson (train): 100%|██████████| 40/40 [18:10<00:00, 27.27s/it]\n",
            "Extracting Arson (test): 100%|██████████| 10/10 [01:41<00:00, 10.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Assault: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Assault (train): 100%|██████████| 40/40 [07:40<00:00, 11.50s/it]\n",
            "Extracting Assault (test): 100%|██████████| 10/10 [01:54<00:00, 11.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER2:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXsIoaT93z9M",
        "outputId": "df75c596-5611-4eb5-9443-327748b9b08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found 6 classes:\n",
            "RoadAccidents: 150 videos\n",
            "Robbery: 150 videos\n",
            "Shooting: 50 videos\n",
            "Shoplifting: 50 videos\n",
            "Stealing: 100 videos\n",
            "Vandalism: 50 videos\n",
            "\n",
            " RoadAccidents: 120 train, 30 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting RoadAccidents (train): 100%|██████████| 120/120 [10:48<00:00,  5.40s/it]\n",
            "Extracting RoadAccidents (test): 100%|██████████| 30/30 [03:25<00:00,  6.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Robbery: 120 train, 30 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Robbery (train): 100%|██████████| 120/120 [21:02<00:00, 10.52s/it]\n",
            "Extracting Robbery (test): 100%|██████████| 30/30 [06:00<00:00, 12.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shooting: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Shooting (train): 100%|██████████| 40/40 [07:26<00:00, 11.16s/it]\n",
            "Extracting Shooting (test): 100%|██████████| 10/10 [01:39<00:00,  9.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shoplifting: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Shoplifting (train): 100%|██████████| 40/40 [16:29<00:00, 24.74s/it]\n",
            "Extracting Shoplifting (test): 100%|██████████| 10/10 [04:08<00:00, 24.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Stealing: 80 train, 20 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Stealing (train): 100%|██████████| 80/80 [21:50<00:00, 16.38s/it]\n",
            "Extracting Stealing (test): 100%|██████████| 20/20 [06:50<00:00, 20.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Vandalism: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Vandalism (train): 100%|██████████| 40/40 [07:15<00:00, 10.89s/it]\n",
            "Extracting Vandalism (test): 100%|██████████| 10/10 [02:21<00:00, 14.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER3:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8rSbImD334F",
        "outputId": "e92861f0-dca0-44b3-b66a-8f5b1a8e6e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found 1 classes:\n",
            "Normal: 50 videos\n",
            "\n",
            " Normal: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Normal (train): 100%|██████████| 40/40 [10:24<00:00, 15.61s/it]\n",
            "Extracting Normal (test): 100%|██████████| 10/10 [00:55<00:00,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER4:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"FightingA\" in class_dir.name:\n",
        "        label= \"Fight\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0rmxv-yvy4u",
        "outputId": "89540ed2-1c7a-4ace-8da5-a035350636c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found 2 classes:\n",
            "Burglary: 50 videos\n",
            "Fight: 50 videos\n",
            "\n",
            " Burglary: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Burglary (train): 100%|██████████| 40/40 [11:58<00:00, 17.96s/it]\n",
            "Extracting Burglary (test): 100%|██████████| 10/10 [02:00<00:00, 12.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Fight: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Fight (train): 100%|██████████| 40/40 [15:27<00:00, 23.20s/it]\n",
            "Extracting Fight (test): 100%|██████████| 10/10 [05:26<00:00, 32.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction pipeline\n",
        "def main():\n",
        "  all_videos= []\n",
        "\n",
        "  # Collect all .mp4 videos\n",
        "  for folder in SOURCE_FOLDER5:\n",
        "    if not folder.exists():\n",
        "      continue\n",
        "    for class_dir in folder.iterdir():\n",
        "      if not class_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "      # Normalize class name\n",
        "      if \"Normal\" in class_dir.name:\n",
        "        label= \"Normal\"\n",
        "      else:\n",
        "        label = class_dir.name.strip()\n",
        "\n",
        "      videos=  list(class_dir.glob(\"*.mp4\"))\n",
        "      for v in videos:\n",
        "        all_videos.append((v,label))\n",
        "\n",
        "  # Group by class and split into train/test\n",
        "  class_to_videos= {}\n",
        "  for path, label in all_videos:\n",
        "    class_to_videos.setdefault(label, []).append(path)\n",
        "\n",
        "  print(f\"\\n Found {len(class_to_videos)} classes:\")\n",
        "  for cls, vids in class_to_videos.items():\n",
        "    print(f\"{cls}: {len(vids)} videos\")\n",
        "\n",
        "  # Extract frames into frames/train/ and frames/test/\n",
        "  for cls, videos in class_to_videos.items():\n",
        "    random.shuffle(videos)\n",
        "    split_idx= int(len(videos) * SPLIT_RATIO)\n",
        "    train_videos= videos[:split_idx]\n",
        "    test_videos= videos[split_idx:]\n",
        "\n",
        "    print(f\"\\n {cls}: {len(train_videos)} train, {len(test_videos)} test\")\n",
        "\n",
        "    # Train set\n",
        "    for idx, video_path in enumerate(tqdm(train_videos, desc=f\"Extracting {cls} (train)\")):\n",
        "      clip_folder= FRAMES_DIR/\"train\"/cls/f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    # Test set\n",
        "    for idx, video_path in enumerate(tqdm(test_videos, desc=f\"Extracting {cls} (test)\")):\n",
        "      clip_folder= FRAMES_DIR / \"test\" / cls / f\"clip_{idx+1:04d}\"\n",
        "      extract_frames(video_path, clip_folder)\n",
        "\n",
        "    print(\"\\n Frame extraction completed successfully!\")\n",
        "    print(f\"Frames saved under: {FRAMES_DIR}\")\n",
        "\n",
        "# RUN\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBFDLDJL7G1u",
        "outputId": "bcfbcc00-6dd5-4ebc-f0e1-5fe10a11fcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Found 2 classes:\n",
            "Explosion: 49 videos\n",
            "Shooting: 50 videos\n",
            "\n",
            " Explosion: 39 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Explosion (train): 100%|██████████| 39/39 [07:35<00:00, 11.69s/it]\n",
            "Extracting Explosion (test): 100%|██████████| 10/10 [01:07<00:00,  6.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n",
            "\n",
            " Shooting: 40 train, 10 test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Shooting (train): 100%|██████████| 40/40 [1:06:21<00:00, 99.53s/it] \n",
            "Extracting Shooting (test): 100%|██████████| 10/10 [23:20<00:00, 140.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Frame extraction completed successfully!\n",
            "Frames saved under: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "shutil.rmtree(FRAMES_DIR / \"train\"/ \"Normal\", ignore_errors=True)"
      ],
      "metadata": {
        "id": "8TkEHwZ227O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "IMG_SIZE= (224, 224)\n",
        "SEQUENCE_LENGTH= 30 #frames per clip"
      ],
      "metadata": {
        "id": "k5ATVndgbbxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ApZcmYTz-WCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FRAMES_DIR = Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames\")   # where your frames live: /content/frames/train, /content/frames/test\n",
        "OUT_DIR    = Path(\"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS\")  # where to save tfrecords (Drive OK, few files)\n",
        "SEQ_LEN    = 30         # frames per clip (pad/trunc)\n",
        "IMG_SIZE   = (224,224)  # resize\n",
        "compress_jpeg = True    # store frames as jpeg bytes inside TFRecord (reduces disk & TFRecord size)"
      ],
      "metadata": {
        "id": "a5NiOJXm7px2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Arm8x0pH8Hyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _bytes_feature(b): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[b]))\n",
        "def _int64_feature(i): return tf.train.Feature(int64_list=tf.train.Int64List(value=[i]))"
      ],
      "metadata": {
        "id": "w-Obga-P8eJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect classes from train folder\n",
        "train_dir = FRAMES_DIR / \"train\"\n",
        "classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "print(\"Detected classes:\", classes)\n",
        "\n",
        "# save classes mapping for later\n",
        "np.save(str(OUT_DIR / \"classes.npy\"), np.array(classes))\n",
        "\n",
        "label_encoder = LabelEncoder().fit(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUS0sXDZ9xGD",
        "outputId": "f0e1309f-b4c4-4143-e1e2-b71c219e6898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected classes: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fight', 'Normal', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_to_tfrecord(clip_path, label_int):\n",
        "    \"\"\"Serialize one clip folder to a tf.train.Example\"\"\"\n",
        "    files = sorted([f for f in os.listdir(clip_path) if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "    frames_bytes = []\n",
        "\n",
        "    for fname in files[:SEQ_LEN]:\n",
        "        img = cv2.imread(str(clip_path / fname))\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, IMG_SIZE)\n",
        "        if compress_jpeg:\n",
        "            ok, enc = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
        "            if not ok:\n",
        "                continue\n",
        "            frames_bytes.append(enc.tobytes())\n",
        "        else:\n",
        "            # raw bytes (float32 is big) -> store uint8 raw\n",
        "            frames_bytes.append(img.tobytes())\n",
        "\n",
        "    # pad by repeating last frame if needed\n",
        "    if len(frames_bytes) == 0:\n",
        "        return None\n",
        "    while len(frames_bytes) < SEQ_LEN:\n",
        "        frames_bytes.append(frames_bytes[-1])\n",
        "\n",
        "    feature = {\n",
        "        \"label\": _int64_feature(int(label_int)),\n",
        "        \"num_frames\": _int64_feature(len(frames_bytes)),\n",
        "        \"height\": _int64_feature(IMG_SIZE[0]),\n",
        "        \"width\": _int64_feature(IMG_SIZE[1]),\n",
        "        \"channels\": _int64_feature(3),\n",
        "        \"clip_name\": _bytes_feature(str(clip_path.name).encode(\"utf-8\")),\n",
        "        # frames as repeated bytes; store concatenated with separator? TF supports bytes_list\n",
        "        \"frames\": tf.train.Feature(bytes_list=tf.train.BytesList(value=frames_bytes)),\n",
        "    }\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example"
      ],
      "metadata": {
        "id": "6uUCagyj9-Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to write TFRecord for one split and one class\n",
        "def write_class_tfrecord(split, cls):\n",
        "    class_dir = FRAMES_DIR / split / cls\n",
        "    if not class_dir.exists():\n",
        "        print(\"Missing:\", class_dir); return\n",
        "    out_file = OUT_DIR / f\"{split}_{cls}.tfrecord\"\n",
        "    if out_file.exists():\n",
        "        print(f\"Skipping (exists): {out_file}\")\n",
        "        return\n",
        "    writer = tf.io.TFRecordWriter(str(out_file))\n",
        "    label_int = int(label_encoder.transform([cls])[0])\n",
        "    clips = sorted([p for p in class_dir.iterdir() if p.is_dir()])\n",
        "    print(f\"Writing {out_file}  ({len(clips)} clips)\")\n",
        "    for clip in tqdm(clips):\n",
        "        ex = clip_to_tfrecord(clip, label_int)\n",
        "        if ex is None:\n",
        "            continue\n",
        "        writer.write(ex.SerializeToString())\n",
        "    writer.close()\n",
        "    print(\"Saved:\", out_file)"
      ],
      "metadata": {
        "id": "EW6F6qF5-G8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write records class-by-class for both splits\n",
        "for split in (\"train\",\"test\"):\n",
        "    for cls in classes:\n",
        "        write_class_tfrecord(split, cls)\n",
        "\n",
        "print(\"All TFRecords created in:\", OUT_DIR)\n",
        "print(\"Saved class list to:\", OUT_DIR / \"classes.npy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nQxY5dr-S6T",
        "outputId": "f43c3abc-7db0-4465-c2ff-2a99ab11dfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Abuse.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Arrest.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Arson.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Assault.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Burglary.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Explosion.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Fight.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Normal.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_RoadAccidents.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Robbery.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Shooting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Shoplifting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Stealing.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/train_Vandalism.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Abuse.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Arrest.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Arson.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Assault.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Burglary.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Explosion.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Fight.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Normal.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_RoadAccidents.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Robbery.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Shooting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Shoplifting.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Stealing.tfrecord\n",
            "Skipping (exists): /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/test_Vandalism.tfrecord\n",
            "All TFRecords created in: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS\n",
            "Saved class list to: /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Frames/ARGUS_TFRECORDS/classes.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High-level flow:\n",
        "\n",
        "1. Read TFRecords with tf.data (parse, decode JPEG bytes → frames tensor).\n",
        "\n",
        "2. Batch & shuffle with tf.data so training reads only small chunks into RAM.\n",
        "\n",
        "3. Apply light augmentation (optional, frame-wise or temporal).\n",
        "\n",
        "4. Model: per-frame CNN (feature extractor) wrapped with TimeDistributed → temporal model (LSTM or GRU) → attention → dense classifier.\n",
        "\n",
        "5. Train with model.fit() on the tf.data datasets, use callbacks (checkpoint, reduce LR).\n",
        "\n",
        "6. Save final weights and small artifacts (classes.npy) to Drive."
      ],
      "metadata": {
        "id": "Ofm29hkpxZED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 30\n",
        "IMG_H, IMG_W = 224, 224\n",
        "AUTOTUNE= tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "Q1PPuzw8xlt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_desc= {\n",
        "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"num_frames\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"channels\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"clip_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"frames\": tf.io.VarLenFeature(tf.string)  # list of jpeg bytes\n",
        "}"
      ],
      "metadata": {
        "id": "pFBLkkpays5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_example(serialized):\n",
        "  ex= tf.io.parse_single_example(serialized, feature_desc)\n",
        "  frames_sparse= ex[\"frames\"] #SparseTensor of bytes\n",
        "  frames= tf.sparse.to_dense(frames_sparse, default_value=b'')\n",
        "  # decode each jpeg\n",
        "  def decode_fn(b):\n",
        "    img= tf.image.decode_jpeg(b, channels=3)\n",
        "    img= tf.image.resize(img, [IMG_H, IMG_W])\n",
        "    img= tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "  frames= tf.map_fn(decode_fn, frames, dtype= tf.float32)\n",
        "  frames= tf.reshape(frames, (SEQ_LEN, IMG_H, IMG_W, 3))\n",
        "  label= tf.cast(ex[\"label\"], tf.int32)\n",
        "  return frames, label"
      ],
      "metadata": {
        "id": "0PzBZbJT2q63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Build train/val datasets with batching and prefetching\n",
        "def make_dataset(tfrecord_paths, num_classes, batch_size=4, shuffle_buffer=256, training=True):\n",
        "    ds = tf.data.TFRecordDataset(tfrecord_paths, num_parallel_reads=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(_parse_example, num_parallel_calls=AUTOTUNE)\n",
        "    # convert int label -> one-hot\n",
        "    ds = ds.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "VK3wJQvIfj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usage of above function\n",
        "classes= np.load(OUT_DIR/ \"classes.npy\")\n",
        "num_classes= len(classes)\n",
        "train_paths= [str(p) for p in OUT_DIR.glob(\"train_*.tfrecord\")]\n",
        "val_paths= [str(p) for p in OUT_DIR.glob(\"test_*tfrecord\")]\n",
        "batch_size = 4\n",
        "train_ds = make_dataset(train_paths,num_classes, batch_size=batch_size, training=True)\n",
        "val_ds   = make_dataset(val_paths, num_classes, batch_size=batch_size, training=False)\n",
        "\n",
        "print(\"Train TFRecords:\", len(train_paths))\n",
        "print(\"Val TFRecords:\", len(val_paths))"
      ],
      "metadata": {
        "id": "gvVlPUWh7vMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d027bd-92e8-4920-82b0-1c1ebefa82d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TFRecords: 14\n",
            "Val TFRecords: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARGUS Architecture\n",
        "Pre-filter (Motion Thresholding & Person Count)  --> CNN (mobilenetV2) --> LSTM (BiLSTM) --> Spatial + Temporal Attention --> Dense Layer (final output of prediction)"
      ],
      "metadata": {
        "id": "04A5T8g4JbCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_argus_model(seq_len=SEQ_LEN, img_size=(IMG_H, IMG_W), num_classes=num_classes, feature_dim= 512):\n",
        "   # 1) Base CNN: MobileNetV2 (frame-level feature extractor)\n",
        "  base_cnn= MobileNetV2(\n",
        "      include_top= False,\n",
        "      pooling= \"avg\",\n",
        "      input_shape= (img_size[0], img_size[1], 3),\n",
        "      weights= \"imagenet\"\n",
        "  )\n",
        "  base_cnn.trainable= False  # start frozen; fine-tune later\n",
        "\n",
        "  # 2) Input: sequence of frames\n",
        "  frames_in= layers.Input(shape=(seq_len, img_size[0], img_size[1], 3), name=\"frames\")\n",
        "\n",
        "  # 3) TimeDistributed CNN: apply MobileNetV2 to each frame\n",
        "  x = layers.TimeDistributed(base_cnn, name=\"frame_cnn\")(frames_in)  # (B, T, cnn_feat_dim)\n",
        "\n",
        "  # project features to lower dimension\n",
        "  x = layers.TimeDistributed(layers.Dense(feature_dim, activation='relu'), name=\"frame_fc\")(x)\n",
        "\n",
        "  # 4) Temporal Modelling using BiLSTM\n",
        "  x= layers.Bidirectional(\n",
        "      layers.LSTM(256, return_sequences=True),\n",
        "      name=\"bilstm\"\n",
        "  )(x)                   # (B, T, 512)\n",
        "\n",
        "  # 5) Attention over time (Temporal Attention)\n",
        "  # score per timestep\n",
        "  attn_scores= layers.Dense(1, activation=\"tanh\")(x)\n",
        "  attn_scores= layers.Flatten()(attn_scores)\n",
        "  attn_weights= layers.Activation(\"softmax\", name=\"attn_weights\")(attn_scores)\n",
        "\n",
        "  # make weights broadcastable: (B, T, 1)\n",
        "  attn_weights = layers.RepeatVector(x.shape[-1])(attn_weights)  # (B, feat, T)\n",
        "  attn_weights = layers.Permute([2, 1])(attn_weights)            # (B, T, feat)\n",
        "\n",
        "  # weighted sum of LSTM outputs\n",
        "  x = layers.Multiply()([x, attn_weights])                       # (B, T, feat)\n",
        "  x = layers.Lambda(lambda t: tf.reduce_sum(t, axis=1), name=\"attn_pool\")(x)  # (B, feat)\n",
        "\n",
        "  # 6) Classification Model\n",
        "  x = layers.Dense(256, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  out = layers.Dense(num_classes, activation='softmax', name=\"predictions\")(x)\n",
        "\n",
        "  model = Model(frames_in, out, name=\"ARGUS_ViolenceDetector\")\n",
        "  return model\n",
        "\n",
        "model = build_argus_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "L0MwLIoSCkVc",
        "outputId": "c9ab59ac-8df1-4246-dbed-9cb598070984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ARGUS_ViolenceDetector\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ARGUS_ViolenceDetector\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  │  \u001b[38;5;34m2,257,984\u001b[0m │ frames[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m655,872\u001b[0m │ frame_cnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,574,912\u001b[0m │ frame_fc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │        \u001b[38;5;34m513\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ attn_weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ permute (\u001b[38;5;33mPermute\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ permute[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ attn_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ predictions (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │      \u001b[38;5;34m3,598\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ frames (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_cnn           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ frames[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ frame_fc            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ frame_cnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ frame_fc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_weights        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attn_weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ permute[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attn_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ attn_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,598</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,624,207\u001b[0m (17.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,624,207</span> (17.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,366,223\u001b[0m (9.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,366,223</span> (9.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Cell\n",
        "(PHASE 1: Backbone Frozen)"
      ],
      "metadata": {
        "id": "JxqaHtLnyVd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import os"
      ],
      "metadata": {
        "id": "uLPtUHkey5ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR= \"/content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "checkpoint_path = os.path.join(SAVE_DIR, \"argus_best_model.h5\")"
      ],
      "metadata": {
        "id": "29z_0PTpy7kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "callbacks=[\n",
        "    ModelCheckpoint(\n",
        "      checkpoint_path,\n",
        "      monitor=\"val_loss\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only= False,\n",
        "      verbose=1\n",
        " ),\n",
        "    EarlyStopping(\n",
        "      monitor=\"val_loss\",\n",
        "      patience=5,\n",
        "      restore_best_weights=True\n",
        " ),\n",
        "    ReduceLROnPlateau(\n",
        "      monitor=\"val_loss\",\n",
        "      factor=0.3,\n",
        "      patience=3,\n",
        "      verbose=1\n",
        " )\n",
        "]"
      ],
      "metadata": {
        "id": "ES66ohqY0H6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ6xPqI92Kgv",
        "outputId": "03d903cb-5f74-4f9e-e086-4ffc802e9d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "    190/Unknown \u001b[1m183s\u001b[0m 319ms/step - accuracy: 0.1960 - loss: 2.4924"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 2.55680, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 605ms/step - accuracy: 0.1963 - loss: 2.4921 - val_accuracy: 0.1623 - val_loss: 2.5568 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.1947 - loss: 2.5788\n",
            "Epoch 2: val_loss improved from 2.55680 to 2.40550, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 408ms/step - accuracy: 0.1952 - loss: 2.5774 - val_accuracy: 0.2618 - val_loss: 2.4055 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.2596 - loss: 2.3728\n",
            "Epoch 3: val_loss improved from 2.40550 to 2.37513, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 410ms/step - accuracy: 0.2601 - loss: 2.3714 - val_accuracy: 0.2827 - val_loss: 2.3751 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.3136 - loss: 2.1054\n",
            "Epoch 4: val_loss improved from 2.37513 to 2.35819, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 409ms/step - accuracy: 0.3141 - loss: 2.1042 - val_accuracy: 0.2827 - val_loss: 2.3582 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.3799 - loss: 1.9445\n",
            "Epoch 5: val_loss improved from 2.35819 to 2.33433, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 415ms/step - accuracy: 0.3804 - loss: 1.9432 - val_accuracy: 0.2513 - val_loss: 2.3343 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.4646 - loss: 1.6522\n",
            "Epoch 6: val_loss did not improve from 2.33433\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 399ms/step - accuracy: 0.4650 - loss: 1.6513 - val_accuracy: 0.2723 - val_loss: 2.3618 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.5100 - loss: 1.4530\n",
            "Epoch 7: val_loss did not improve from 2.33433\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 398ms/step - accuracy: 0.5105 - loss: 1.4521 - val_accuracy: 0.2670 - val_loss: 2.4415 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.6229 - loss: 1.2072\n",
            "Epoch 8: val_loss did not improve from 2.33433\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 397ms/step - accuracy: 0.6232 - loss: 1.2064 - val_accuracy: 0.2984 - val_loss: 2.4626 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.7273 - loss: 0.9331\n",
            "Epoch 9: val_loss did not improve from 2.33433\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 401ms/step - accuracy: 0.7277 - loss: 0.9320 - val_accuracy: 0.3089 - val_loss: 2.3957 - learning_rate: 3.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.8232 - loss: 0.6621\n",
            "Epoch 10: val_loss did not improve from 2.33433\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 400ms/step - accuracy: 0.8235 - loss: 0.6615 - val_accuracy: 0.3037 - val_loss: 2.4899 - learning_rate: 3.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Phase 2: Unfreeze backbone + fine-tune)"
      ],
      "metadata": {
        "id": "tYaC4sRc2WOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_cnn = model.get_layer(\"frame_cnn\").layer\n",
        "base_cnn.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTEnV7FV7-N0",
        "outputId": "46852794-4fc6-4f20-cf0f-ec7997b6f2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "    190/Unknown \u001b[1m621s\u001b[0m 1s/step - accuracy: 0.1666 - loss: 2.6666\n",
            "Epoch 1: val_loss improved from 2.33433 to 2.32694, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 2s/step - accuracy: 0.1672 - loss: 2.6649 - val_accuracy: 0.2723 - val_loss: 2.3269 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2089 - loss: 2.4830\n",
            "Epoch 2: val_loss improved from 2.32694 to 2.29769, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 1s/step - accuracy: 0.2093 - loss: 2.4819 - val_accuracy: 0.2827 - val_loss: 2.2977 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2214 - loss: 2.3651\n",
            "Epoch 3: val_loss improved from 2.29769 to 2.28731, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 1s/step - accuracy: 0.2220 - loss: 2.3640 - val_accuracy: 0.2723 - val_loss: 2.2873 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2390 - loss: 2.3189\n",
            "Epoch 4: val_loss improved from 2.28731 to 2.28347, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 1s/step - accuracy: 0.2394 - loss: 2.3179 - val_accuracy: 0.2775 - val_loss: 2.2835 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2528 - loss: 2.2734\n",
            "Epoch 5: val_loss improved from 2.28347 to 2.27323, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 1s/step - accuracy: 0.2533 - loss: 2.2722 - val_accuracy: 0.2565 - val_loss: 2.2732 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2489 - loss: 2.2491\n",
            "Epoch 6: val_loss improved from 2.27323 to 2.25952, saving model to /content/drive/MyDrive/DetectionWithDroneModel(ARGUS)/VideoDetectionDataset/Drone_Detection_Dataset(Unzipped Files)/Model_Trained/argus_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 1s/step - accuracy: 0.2495 - loss: 2.2479 - val_accuracy: 0.2670 - val_loss: 2.2595 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3481 - loss: 2.0329\n",
            "Epoch 7: val_loss did not improve from 2.25952\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 1s/step - accuracy: 0.3485 - loss: 2.0318 - val_accuracy: 0.2618 - val_loss: 2.2817 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3281 - loss: 2.0511\n",
            "Epoch 8: val_loss did not improve from 2.25952\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.3285 - loss: 2.0500 - val_accuracy: 0.2565 - val_loss: 2.2922 - learning_rate: 1.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3689 - loss: 1.9923\n",
            "Epoch 9: val_loss did not improve from 2.25952\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.9999999242136253e-06.\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 0.3693 - loss: 1.9912 - val_accuracy: 0.2408 - val_loss: 2.3094 - learning_rate: 1.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3858 - loss: 1.9125\n",
            "Epoch 10: val_loss did not improve from 2.25952\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.3862 - loss: 1.9114 - val_accuracy: 0.2670 - val_loss: 2.3073 - learning_rate: 3.0000e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3937 - loss: 1.9618\n",
            "Epoch 11: val_loss did not improve from 2.25952\n",
            "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.3942 - loss: 1.9604 - val_accuracy: 0.2670 - val_loss: 2.2977 - learning_rate: 3.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJYBn2tu8FQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}